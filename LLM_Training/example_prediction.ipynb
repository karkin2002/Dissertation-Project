{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bb2884884b72db",
   "metadata": {},
   "source": [
    "# Prediction After Fine-Tuning - Simple Example.\n",
    "\n",
    "Date\n",
    ": 06/03/2025\n",
    "\n",
    "Author\n",
    ": Kaya Arkin (Stu No. 2105361)\n",
    "\n",
    "Copyright\n",
    ": Swansea University\n",
    "\n",
    "### **Context**\n",
    "This jupyter notebook is authored by Kaya Arkin (Stu No. 2105361) as part of their Swansea University final year project *\"Exploratory Research on Explainable LLMs (Airbus AI Research)\"*. The project is supervised by Mark Hall, an employee at the Airbus AI Research Department, and Bertie Muller, a university assigned supervisor. The project aims *\"to provide an insightful set of findings and recommendations on fine-turned local explanations for LLMs that can utilised as resource for future explainability implementations\"* tailored towards Airbus AI Research.\n",
    "\n",
    "Please ensure you have read `README.txt` before continuing.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Explanation**\n",
    "\n",
    "THe jupyter notebook provides an example of the fine-tuned LLM predicting a part failure for a given report. In this example, the fine-tuned model is loaded, an input is tokenised and fed into the model which outputs a predicted response.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d8d33fef92d8d",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33405d7ca1fe1f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:15:28.875883Z",
     "start_time": "2025-03-06T12:15:28.873718Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0d9e387d146f19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Hardware Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129eda1c8d750022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:15:28.919852Z",
     "start_time": "2025-03-06T12:15:28.917906Z"
    }
   },
   "outputs": [],
   "source": [
    "## Hardware Setup Constants\n",
    "CPU_DEVICE_NAME = \"cpu\"\n",
    "GPU_DEVICE_NAME = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e4608af6af1a570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:15:28.969768Z",
     "start_time": "2025-03-06T12:15:28.964310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU processing enabled.\n"
     ]
    }
   ],
   "source": [
    "## Set processing to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = GPU_DEVICE_NAME\n",
    "\n",
    "    ## Empty GPU VRAM\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"GPU processing enabled.\")\n",
    "\n",
    "else:\n",
    "    device = CPU_DEVICE_NAME\n",
    "    print(\"GPU processing not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd47f62e4d3495e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Preprocessing Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3b1c8eeb790c6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:15:29.070255Z",
     "start_time": "2025-03-06T12:15:29.068157Z"
    }
   },
   "outputs": [],
   "source": [
    "## Preprocessing Constants\n",
    "MODEL_PATH = \"./t5_finetuned_airline_incidents\"\n",
    "INPUT_FORMAT = \"Report: {report}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6e39ffaca40310",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:15:29.606864Z",
     "start_time": "2025-03-06T12:15:29.112671Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './t5_finetuned_airline_incidents'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\karki\\.conda\\envs\\bert_venv\\Lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[0;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\karki\\.conda\\envs\\bert_venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"repo_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"from_id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"to_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\karki\\.conda\\envs\\bert_venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mREPO_ID_REGEX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         raise HFValidationError(\n\u001b[0m\u001b[0;32m    161\u001b[0m             \u001b[1;34m\"Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './t5_finetuned_airline_incidents'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20080\\4104180523.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\karki\\.conda\\envs\\bert_venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2130\u001b[0m                     \u001b[1;31m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2131\u001b[0m                     \u001b[0mfast_tokenizer_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFULL_TOKENIZER_FILE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2132\u001b[1;33m                     resolved_config_file = cached_file(\n\u001b[0m\u001b[0;32m   2133\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m                         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\karki\\.conda\\envs\\bert_venv\\Lib\\site-packages\\transformers\\utils\\hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33mThere was a specific connection error when trying to load \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\\n\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m         raise EnvironmentError(\n\u001b[0m\u001b[0;32m    470\u001b[0m             \u001b[1;33mf\"\u001b[0m\u001b[1;33mIncorrect path_or_model_id: '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Incorrect path_or_model_id: './t5_finetuned_airline_incidents'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983248b14c62e2b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:15:29.618749Z",
     "start_time": "2025-03-06T12:15:29.616102Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BatchEncoding ## This import is only used for python annotations, not implementation\n",
    "\n",
    "def tokenise_input(report: str) -> BatchEncoding:\n",
    "    input_text = INPUT_FORMAT.format(report=report)\n",
    "    return tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24419adf6a2a05de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Generate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf404ba2841a553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:15:29.661124Z",
     "start_time": "2025-03-06T12:15:29.658567Z"
    }
   },
   "outputs": [],
   "source": [
    "## Generating Prediction Constants\n",
    "EXAMPLE_REPORT = \"THE FLIGHT CREW REPORTED OF A BAGGAGE/FUEL DOOR CAS MESSAGE. OPERATIONS WERE CONTINUED PER MEL 52-4. THE MAINTENANCE TEAM TROUBLESHOT AND FOUND THE FUEL DOOR MICROSWITCH TO BE DEFECTIVE. THE SWITCH WAS REPLACED WITH NEW AND RIGGED. OPERATIONS WERE SATISFACTORY AND THE AIRCRAFT WAS RETURNED TO SERVICE.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ed5d4553bb32a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:15:29.712642Z",
     "start_time": "2025-03-06T12:15:29.709879Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_predicted_part_failure(report: str) -> str:\n",
    "    inputs = tokenise_input(report)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=128)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f9358e8063aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:15:29.870012Z",
     "start_time": "2025-03-06T12:15:29.755555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report: THE FLIGHT CREW REPORTED OF A BAGGAGE/FUEL DOOR CAS MESSAGE. OPERATIONS WERE CONTINUED PER MEL 52-4. THE MAINTENANCE TEAM TROUBLESHOT AND FOUND THE FUEL DOOR MICROSWITCH TO BE DEFECTIVE. THE SWITCH WAS REPLACED WITH NEW AND RIGGED. OPERATIONS WERE SATISFACTORY AND THE AIRCRAFT WAS RETURNED TO SERVICE.\n",
      "Part Failure: FUEL DOOR DEFECTIVE\n"
     ]
    }
   ],
   "source": [
    "print(\"Report:\", EXAMPLE_REPORT)\n",
    "print(get_predicted_part_failure(EXAMPLE_REPORT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355ea8be9edf122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:38:02.654224Z",
     "start_time": "2025-03-06T12:38:02.544817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report: THE FLIGHT CREW REPORTED OF A BAGGAGE/FUEL DOOR CAS MESSAGE. OPERATIONS WERE CONTINUED PER MEL 52-4. THE MAINTENANCE TEAM TROUBLESHOT AND FOUND THE FUEL DOOR MICROSWITCH TO BE DEFECTIVE. THE SWITCH WAS REPLACED WITH NEW AND RIGGED. OPERATIONS WERE SATISFACTORY AND THE AIRCRAFT WAS RETURNED TO SERVICE.\n",
      "Part Failure: FUEL DOOR DEFECTIVE\n"
     ]
    }
   ],
   "source": [
    "new_report = \"THE FLIGHT CREW REPORTED OF A BAGGAGE/FUEL DOOR CAS MESSAGE. OPERATIONS WERE CONTINUED PER MEL 52-4. THE MAINTENANCE TEAM TROUBLESHOT AND FOUND THE FUEL DOOR MICROSWITCH TO BE DEFECTIVE. THE SWITCH WAS REPLACED WITH NEW AND RIGGED. OPERATIONS WERE SATISFACTORY AND THE AIRCRAFT WAS RETURNED TO SERVICE.\"\n",
    "print(\"Report:\", new_report)\n",
    "print(get_predicted_part_failure(new_report))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
