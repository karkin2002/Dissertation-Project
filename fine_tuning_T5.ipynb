{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Fine-Tuning DistilBERT on the Federal Aviation Administration (FAA) Service Difficulty Report (SDR).\n",
    "\n",
    "Date\n",
    ": 05/03/2025\n",
    "\n",
    "Author\n",
    ": Kaya Arkin (Stu No. 2105361)\n",
    "\n",
    "Copyright\n",
    ": Swansea University\n",
    "\n",
    "### **Context**\n",
    "This jupyter notebook is authored by Kaya Arkin (Stu No. 2105361) as part of their Swansea University final year project *\"Exploratory Research on Explainable LLMs (Airbus AI Research)\"*. The project is supervised by Mark Hall, an employee at the Airbus AI Research Department, and Bertie Muller, a university assigned supervisor. The project aims *\"to provide an insightful set of findings and recommendations on fine-turned local explanations for LLMs that can utilised as resource for future explainability implementations\"* tailored towards Airbus AI Research.\n",
    "\n",
    "Please ensure you have read `README.txt` before continuing.\n",
    "\n",
    "---\n",
    "\n",
    "### **Code Explanation**\n",
    "\n",
    "The jupyter notebook aims to finetune a DistilBERT model on an Airlines Incidents dataset (FAA SDR).\n",
    "\n",
    "#### What does the code do?\n",
    "1. Loads the Airlines Incident Report Dataset.\n",
    "2. Formats the data.\n",
    "3. Fine-tunes a DistilBERT model using T5.\n",
    "4. Saves the pre-trained model.\n",
    "\n",
    "#### Why is DistilBERT used?\n",
    "1. Abundance of online documentation, resources, & guidance.\n",
    "2. 40% less parameters, 60% faster, and 95% performance of BERT; allowing for quicker development iterations and enables model to run on lower-end hardware.\n",
    "3. Compatibility & customise-ability with other python libraries.\n",
    "\n",
    "#### Why is T5 (Text-to-Text Transfer Transformer) used to train the model?\n",
    "The scenario for our LLM is that for a given (unseen) incident report the LLM predicts the part failure. For example, inputting the incident *\"FLIGHT CREW REPORTED OF A BAGGAGE/FUEL DOOR CAS MESSAGE ...\"* the LLM should return a predicted part failure, *\"FUEL DOOR DEFECTIVE\"*.\n",
    "\n",
    "Originally, I treated this tasks as a classification problem. However, as the dataset contains over 10,000 unique part failure and the dataset does not contain all possible part failures, it wasn't a suitable method. T5 uses a *\"text-to-text\"* paradigm. It treats all tasks, including classification, as text generation. Inputs are formatted with prefixes, like *\"Report: {text}\"*, and outputs are textual labels (e.g. *\"FUEL DOOR DEFECTIVE\"*). T5 allows the model to generate failure types which the LLM was not explicitly trained on.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Imports\n",
    "\n",
    "See `README.txt` for installation requirements."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:40:16.588888Z",
     "start_time": "2025-03-06T11:40:11.902436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch, os\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_from_disk, Dataset\n",
    "import pandas as pd"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arkin/python-venvs/bert-counterfactual/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 2. Hardware Setup\n",
    "\n",
    "1. Checking whether a GPU is available for processing, otherwise defaulting to the cpu for processing.\n",
    "2. Clearing the gpu cache.\n",
    "3. Optimise GPU for matrix multiplications (required as part of T5)\n",
    "\n",
    "Note: For performance reasons GPU processing is highly recommended.\n",
    "\n",
    "See `README.txt` for information on hardware requirements."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:40:16.599761Z",
     "start_time": "2025-03-06T11:40:16.597457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Hardware Setup Constants\n",
    "CPU_DEVICE_NAME = \"cpu\"\n",
    "GPU_DEVICE_NAME = \"cuda\"\n",
    "TORCH_MATRIX_MULTIPLICATION_PRECISION = \"high\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:40:20.221748Z",
     "start_time": "2025-03-06T11:40:16.723892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Set processing to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = GPU_DEVICE_NAME\n",
    "\n",
    "    ## Empty GPU VRAM\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ## Optimises matrix multiplications\n",
    "    ## \"high\" - ensures high precision (closer to the true float 32)\n",
    "    torch.set_float32_matmul_precision(TORCH_MATRIX_MULTIPLICATION_PRECISION)\n",
    "\n",
    "    print(\"GPU processing enabled.\")\n",
    "\n",
    "else:\n",
    "    device = CPU_DEVICE_NAME\n",
    "    print(\"GPU processing not available.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing set to: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 3. Preprocessing Dataset\n",
    "The following code pre-processes the dataset for fine-tuning using T5. We preform the following:\n",
    "1. Initialise the T5 Tokeniser & model.\n",
    "2. Load the dataset.\n",
    "3. Tokenise the inputs & outputs for T5.\n",
    "4. Split the dataset into training & testing.\n",
    "5. Setup the input / output sequences in a batch to be dynamically padded during training."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:40:20.233865Z",
     "start_time": "2025-03-06T11:40:20.230489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Preprocessing Constants\n",
    "MODEL_NAME = \"t5-small\"\n",
    "MODEL_INPUT = \"Report: {input_text}\"\n",
    "MODEL_OUTPUT = \"Part Failure: {output_text}\"\n",
    "MODEL_INPUT_MAX_LENGTH = 512\n",
    "MODEL_OUTPUT_MAX_LENGTH = 128\n",
    "DATASET_REPORT_COLUMN_TITLE = \"report\"\n",
    "DATASET_PART_FAILURE_COLUMN_TITLE = \"part failure\"\n",
    "DATASET_PATH = \"airline_incidents.csv\"\n",
    "PREPROCESSED_DATASET_NANE = \"processed_dataset\"\n",
    "PREPROCESSED_DATASET_PATH = f\"./{PREPROCESSED_DATASET_NANE}\"\n",
    "TRAINING_TEST_SPLIT_RATIO = 0.2"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The following code initialises a T5 tokenizer / model and moves the model to the specified device (either the CPU or GPU).\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:40:21.855556Z",
     "start_time": "2025-03-06T11:40:20.295535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This function preprocesses a single dataset example by formatting input and output text using predefined templates, tokenizing them for the T5 model, and setting the tokenized labels for training."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:40:21.867632Z",
     "start_time": "2025-03-06T11:40:21.864117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_dataset(example):\n",
    "    ## Tokenizes input and output text for T5\n",
    "    model_input = MODEL_INPUT.format(input_text = example[DATASET_REPORT_COLUMN_TITLE])\n",
    "    target_text = MODEL_OUTPUT.format(output_text = example[DATASET_PART_FAILURE_COLUMN_TITLE])\n",
    "\n",
    "    model_inputs = tokenizer(model_input, max_length=MODEL_INPUT_MAX_LENGTH, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(target_text, max_length=MODEL_OUTPUT_MAX_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]  # Assign decoder labels\n",
    "\n",
    "    return model_inputs"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code checks if a preprocessed dataset exists at the specified path (`PREPROCESSED_DATASET_PATH`). If it exists, it loads the dataset. Otherwise, it loads a raw CSV file (`DATASET_PATH`), removes missing values, shuffles the data, converts it into a `Dataset` object, applies a preprocessing function (`preprocess_dataset`) to format and tokenize it, and saves the preprocessed dataset to disk."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:41:32.284217Z",
     "start_time": "2025-03-06T11:40:21.908743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if os.path.exists(PREPROCESSED_DATASET_PATH):\n",
    "    dataset = load_from_disk(PREPROCESSED_DATASET_PATH)\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(DATASET_PATH)  # Ensure the file is in the same directory\n",
    "    df = df.dropna()  # Remove missing values\n",
    "    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle dataset\n",
    "\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.map(preprocess_dataset, remove_columns=[DATASET_REPORT_COLUMN_TITLE, DATASET_PART_FAILURE_COLUMN_TITLE])\n",
    "\n",
    "    dataset.save_to_disk(PREPROCESSED_DATASET_NANE)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100028/100028 [01:09<00:00, 1440.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100028/100028 [00:00<00:00, 227180.61 examples/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code splits the dataset into training and testing subsets, with 80% used for training and 20% used for testing, based on the defined split ratio."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:41:32.351489Z",
     "start_time": "2025-03-06T11:41:32.323711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train-Test Split\n",
    "dataset = dataset.train_test_split(test_size=TRAINING_TEST_SPLIT_RATIO)  # 80% Training, 20% Validation"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The purpose of this code is to ensure that input and output sequences in a batch are padded dynamically during training, making them uniform in length, which is necessary for efficient processing by the model."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:41:32.382767Z",
     "start_time": "2025-03-06T11:41:32.380746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Collator (Pads batch inputs dynamically)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 4. Training DistilBERT Model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code initializes the hyperparameters and configuration required for training the T5 model using the `TrainingArguments` class from the Transformers library. It specifies settings such as batch sizes, gradient accumulation, mixed precision (fp16), learning rate, weight decay, checkpoint saving strategies, logging, and evaluation intervals."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:41:32.431639Z",
     "start_time": "2025-03-06T11:41:32.423994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"../t5_airline_incidents\",\n",
    "    per_device_train_batch_size= 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    gradient_accumulation_steps = 1,  # Helps with large models\n",
    "    bf16 = False,\n",
    "    fp16 = True,\n",
    "    save_total_limit = 2,  # Manage checkpoints\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate = 1e-3,\n",
    "    weight_decay = 0.01,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_torch_fused\",  # Optimized optimizer for ROCm\n",
    "    report_to = \"none\",\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = 100,  # Log every 100 steps\n",
    "    eval_steps = 500  # Evaluate every 500 training steps\n",
    "\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arkin/python-venvs/bert-counterfactual/lib64/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This code initializes a `Trainer` object from the Transformers library, setting up the T5 model, training arguments, training and evaluation datasets, tokenizer, and data collator for model fine-tuning."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:41:32.487317Z",
     "start_time": "2025-03-06T11:41:32.472465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19062/539262303.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:10:45.001479Z",
     "start_time": "2025-03-06T11:41:32.522638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Start Training\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arkin/python-venvs/bert-counterfactual/lib64/python3.11/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:310.)\n",
      "  return F.linear(input, self.weight, self.bias)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3753' max='3753' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3753/3753 29:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.037569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.032731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.031245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3753, training_loss=0.0649011887901788, metrics={'train_runtime': 1752.3425, 'train_samples_per_second': 136.997, 'train_steps_per_second': 2.142, 'total_flos': 3.249096491217715e+16, 'train_loss': 0.0649011887901788, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:10:45.793521Z",
     "start_time": "2025-03-06T12:10:45.029198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SAVE THE FINE-TUNED MODEL\n",
    "model.save_pretrained(\"./t5_finetuned_airline_incidents\")\n",
    "tokenizer.save_pretrained(\"./t5_finetuned_airline_incidents\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./t5_finetuned_airline_incidents/tokenizer_config.json',\n",
       " './t5_finetuned_airline_incidents/special_tokens_map.json',\n",
       " './t5_finetuned_airline_incidents/spiece.model',\n",
       " './t5_finetuned_airline_incidents/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
